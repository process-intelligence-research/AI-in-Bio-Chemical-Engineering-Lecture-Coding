{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 2 - Surrogate modelling example\n",
    "## Artificial neural networks for regression and classification\n",
    "\n",
    "### 1 - Learning objectives \n",
    "\n",
    "After successfully completing this lecture, you are able toâ€¦.\n",
    "* Explain the fundamentals on artificial neural networks along with some of their limitations\n",
    "* Explain the fundamentals on training with backpropagation\n",
    "* Analyse a machine learning result in the context of overfitting\n",
    "* Develop and implement neural networks in the context of basic (Bio)Chemical Engineering applications (i.e., regression and classification) \n",
    "\n",
    "\n",
    "### 2 - Setting up the environment\n",
    "\n",
    "Opposite to Matlab, we do not have all functionalities pre installed in Python. Therefore, the first step for a project is always to install packages which extend our function library. Here, we install the following packages:\n",
    "- *torch*: The PyTorch package is the most important one for this lab and inherits a variety of useful functions for machine learning.\n",
    "- *matplotlib*: Provides plotting functions similar to matlab.\n",
    "- *tqdm*: Allows to print progress bar of for loops\n",
    "- *scikit-learn*: Machine-learning package in python, suitable for preprocessing our datasets\n",
    "- *optim* is a library of optimizers for the backpropagation.\n",
    "\n",
    "Here we use [PyTorch](https://pytorch.org/) to work with neural networks. Pytorch is an open source machine learning framework with many predefined functions which make the work with machine learning way easier. You can learn pytorch from the [tutorial link](https://pytorch.org/tutorials/). The Docs information can be searched at [Docs](https://pytorch.org/docs/stable/index.html). A popular alternative to Pytorch is Keras (building on Tensorflow). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in c:\\users\\hamaldonadodel\\appdata\\roaming\\python\\python36\\site-packages (1.10.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\hamaldonadodel\\appdata\\roaming\\python\\python36\\site-packages (1.1.5)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hamaldonadodel\\appdata\\roaming\\python\\python36\\site-packages (4.64.1)\n",
      "Requirement already satisfied: matplotlib in d:\\programswd\\anaconda\\envs\\tensorflow\\lib\\site-packages (3.3.4)\n",
      "Requirement already satisfied: scikit-learn in d:\\programswd\\anaconda\\envs\\tensorflow\\lib\\site-packages (0.24.2)\n",
      "Requirement already satisfied: seaborn in c:\\users\\hamaldonadodel\\appdata\\roaming\\python\\python36\\site-packages (0.11.2)\n",
      "Requirement already satisfied: dataclasses in d:\\programswd\\anaconda\\envs\\tensorflow\\lib\\site-packages (from torch) (0.8)\n",
      "Requirement already satisfied: typing-extensions in d:\\programswd\\anaconda\\envs\\tensorflow\\lib\\site-packages (from torch) (4.1.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in d:\\programswd\\anaconda\\envs\\tensorflow\\lib\\site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: numpy>=1.15.4 in d:\\programswd\\anaconda\\envs\\tensorflow\\lib\\site-packages (from pandas) (1.19.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in d:\\programswd\\anaconda\\envs\\tensorflow\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: colorama in d:\\programswd\\anaconda\\envs\\tensorflow\\lib\\site-packages (from tqdm) (0.4.4)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\hamaldonadodel\\appdata\\roaming\\python\\python36\\site-packages (from tqdm) (5.4.0)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\programswd\\anaconda\\envs\\tensorflow\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in d:\\programswd\\anaconda\\envs\\tensorflow\\lib\\site-packages (from matplotlib) (8.3.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in d:\\programswd\\anaconda\\envs\\tensorflow\\lib\\site-packages (from matplotlib) (3.0.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in d:\\programswd\\anaconda\\envs\\tensorflow\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\programswd\\anaconda\\envs\\tensorflow\\lib\\site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in d:\\programswd\\anaconda\\envs\\tensorflow\\lib\\site-packages (from scikit-learn) (1.5.3)\n",
      "Requirement already satisfied: joblib>=0.11 in d:\\programswd\\anaconda\\envs\\tensorflow\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: six>=1.5 in d:\\programswd\\anaconda\\envs\\tensorflow\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in d:\\programswd\\anaconda\\envs\\tensorflow\\lib\\site-packages (from importlib-resources->tqdm) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "# install dependencies\n",
    "!pip install torch pandas tqdm matplotlib scikit-learn seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # numerical calculations in python\n",
    "import pandas as pd # data analysis library\n",
    "import matplotlib.pyplot as plt  # plotting similar to matlab\n",
    "import torch  # PyTorch: the general machine learning framework in Python\n",
    "import torch.optim as optim  # contains optimizers for the backpropagation\n",
    "import torch.nn as nn  # the artificial neural network module in PyTorch\n",
    "from tqdm import tqdm  # produces progress bars for for-loops in Python\n",
    "from sklearn.model_selection import train_test_split  # randomly splits a dataset\n",
    "from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler, RobustScaler  # scaling algorithm for preprocessing\n",
    "import seaborn as sns # Statistical data visualization package\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\" #Forces pytorch to use CPU, ensuring reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3- Ensuring reproducibility - Setting up the seed \n",
    "\n",
    "To make our results reproducible, we need to set a so-called \"seed\". Machine Learning includes stochastic processes in the weight/bias initialization and the backpropagation. Also the random number generation which we will use for the dataset is a stochastic process. By setting a seed in the program we make sure that always the same random numbers are chosen. Otherwise, we would get different results everytime we run this script, which is not nice for teaching purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up a seed \n",
    "\n",
    "seed = 2024\n",
    "torch.manual_seed(seed)                     # Sets seed for pytorch\n",
    "torch.cuda.manual_seed(seed)                # Sets seed for cuda\n",
    "np.random.seed(seed)                        # Sets seed for random number generator\n",
    "torch.backends.cudnn.deterministic = True   # Forces cuda to use deterministic approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Run</th>\n",
       "      <th>P_{T}</th>\n",
       "      <th>H_{2}:CO</th>\n",
       "      <th>U/W</th>\n",
       "      <th>X_{CO}</th>\n",
       "      <th>LG</th>\n",
       "      <th>LPG</th>\n",
       "      <th>Gas</th>\n",
       "      <th>Dies</th>\n",
       "      <th>O_{2+}</th>\n",
       "      <th>O_{4+}</th>\n",
       "      <th>O_{9+}</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>46.2</td>\n",
       "      <td>0.226</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.339</td>\n",
       "      <td>0.055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>44.6</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.443</td>\n",
       "      <td>0.089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>62.3</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.345</td>\n",
       "      <td>0.048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>68.8</td>\n",
       "      <td>0.282</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.256</td>\n",
       "      <td>0.019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>47.6</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.336</td>\n",
       "      <td>0.035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.5</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.346</td>\n",
       "      <td>0.051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>48.9</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.204</td>\n",
       "      <td>0.463</td>\n",
       "      <td>0.062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>2.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>80.6</td>\n",
       "      <td>0.287</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.156</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>2.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>52.7</td>\n",
       "      <td>0.253</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.177</td>\n",
       "      <td>0.283</td>\n",
       "      <td>0.026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>2.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>61.2</td>\n",
       "      <td>0.269</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.169</td>\n",
       "      <td>0.263</td>\n",
       "      <td>0.021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>53.4</td>\n",
       "      <td>0.283</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>40.8</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.296</td>\n",
       "      <td>0.032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>1.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>62.2</td>\n",
       "      <td>0.334</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.152</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>1.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>33.3</td>\n",
       "      <td>0.279</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.304</td>\n",
       "      <td>0.033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.204</td>\n",
       "      <td>0.431</td>\n",
       "      <td>0.050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>52.7</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.415</td>\n",
       "      <td>0.073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>53.0</td>\n",
       "      <td>0.298</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>1.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>73.0</td>\n",
       "      <td>0.353</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.199</td>\n",
       "      <td>0.014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>45.3</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.378</td>\n",
       "      <td>0.032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>55.3</td>\n",
       "      <td>0.299</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.299</td>\n",
       "      <td>0.025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Run  P_{T}  H_{2}:CO  U/W  X_{CO}     LG    LPG    Gas   Dies  O_{2+}  \\\n",
       "0     1    1.5       2.0  1.5    46.2  0.226  0.069  0.101  0.070   0.140   \n",
       "1     2    1.2       0.5  1.0    44.6  0.133  0.040  0.065  0.049   0.180   \n",
       "2     3    1.6       1.0  1.0    62.3  0.191  0.072  0.103  0.087   0.153   \n",
       "3     4    2.4       2.0  1.0    68.8  0.282  0.110  0.142  0.065   0.126   \n",
       "4     5    1.5       2.0  1.5    47.6  0.239  0.078  0.090  0.063   0.158   \n",
       "5     6    3.2       1.0  1.0    64.5  0.176  0.083  0.103  0.077   0.164   \n",
       "6     7    3.0       0.5  1.0    48.9  0.126  0.055  0.071  0.019   0.204   \n",
       "7     8    2.4       2.0  0.5    80.6  0.287  0.124  0.156  0.108   0.117   \n",
       "8     9    2.4       2.0  2.0    52.7  0.253  0.093  0.101  0.067   0.177   \n",
       "9    10    2.4       2.0  1.5    61.2  0.269  0.101  0.109  0.068   0.169   \n",
       "10   11    1.5       2.0  1.5    53.4  0.283  0.096  0.098  0.054   0.178   \n",
       "11   12    1.5       2.0  1.5    40.8  0.273  0.076  0.080  0.054   0.189   \n",
       "12   13    1.2       2.0  0.5    62.2  0.334  0.098  0.133  0.065   0.152   \n",
       "13   14    1.2       2.0  2.0    33.3  0.279  0.065  0.069  0.048   0.202   \n",
       "14   15    2.4       0.5  2.0    33.0  0.149  0.051  0.061  0.054   0.204   \n",
       "15   16    2.4       0.5  0.5    52.7  0.138  0.052  0.082  0.058   0.181   \n",
       "16   17    1.5       2.0  1.5    53.0  0.298  0.084  0.092  0.057   0.170   \n",
       "17   18    1.2       2.0  0.5    73.0  0.353  0.111  0.124  0.070   0.129   \n",
       "18   19    0.8       1.0  1.0    45.3  0.217  0.055  0.080  0.052   0.187   \n",
       "19   20    1.5       2.0  1.5    55.3  0.299  0.084  0.099  0.029   0.166   \n",
       "\n",
       "    O_{4+}  O_{9+}  \n",
       "0    0.339   0.055  \n",
       "1    0.443   0.089  \n",
       "2    0.345   0.048  \n",
       "3    0.256   0.019  \n",
       "4    0.336   0.035  \n",
       "5    0.346   0.051  \n",
       "6    0.463   0.062  \n",
       "7    0.190   0.018  \n",
       "8    0.283   0.026  \n",
       "9    0.263   0.021  \n",
       "10   0.276   0.016  \n",
       "11   0.296   0.032  \n",
       "12   0.223   0.016  \n",
       "13   0.304   0.033  \n",
       "14   0.431   0.050  \n",
       "15   0.415   0.073  \n",
       "16   0.276   0.024  \n",
       "17   0.199   0.014  \n",
       "18   0.378   0.032  \n",
       "19   0.299   0.025  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the dataset\n",
    "datafile = './FTS_exp_results_Fernandez_et_al.csv'\n",
    "fts_data = pd.read_csv(datafile, sep=',', header=0)\n",
    "fts_data\n",
    "\n",
    "# In case you want to visualize the pairplots\n",
    "#sns.pairplot(data=fts_data)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - Preprocessing data \n",
    "\n",
    "Preprocessing the data that will be analyzed by the ANN is a crucial step. For this we need to perform the following actions: \n",
    "\n",
    "* We need to extract the inputs and outputs of our model from the dataset available. \n",
    "* Split the dataset into the training and test one\n",
    "* Normalize our data for a better handling by the NN\n",
    "* Create the tensors for the training and testing of the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting data\n",
    "x_all = fts_data[[\"P_{T}\",\t\"H_{2}:CO\", \"U/W\"]].values\n",
    "y_all = fts_data[[\"X_{CO}\", \"LG\",\t\"LPG\",\t\"Gas\",\t\"Dies\",\t\"O_{2+}\", \"O_{4+}\",\t\"O_{9+}\"]].values\n",
    "\n",
    "# Data splitting\n",
    "frac_train = 0.70  # Ratio of first splitting of dataset (15 train / 5 test)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_all, y_all, train_size = frac_train, random_state=seed)\n",
    "\n",
    "# Data normalization\n",
    "\n",
    "# Creating the normalization functions for the entries and outputs of the network\n",
    "norm_entries = MinMaxScaler().fit(x_train)\n",
    "norm_output = MinMaxScaler().fit(y_train)\n",
    "\n",
    "# Implementing normalization functions to the datasets\n",
    "norm_x_train = norm_entries.transform(x_train)\n",
    "norm_y_train = norm_output.transform(y_train)\n",
    "norm_x_test = norm_entries.transform(x_test)\n",
    "norm_y_test = norm_output.transform(y_test)\n",
    "\n",
    "# Creating the tensors for the training and test datasets\n",
    "dtype = torch.float\n",
    "xt_train = torch.tensor(norm_x_train, dtype=dtype).float()\n",
    "xt_test = torch.tensor(norm_x_test, dtype=dtype).float()\n",
    "yt_train = torch.tensor(norm_y_train, dtype=dtype).float()\n",
    "yt_test = torch.tensor(norm_y_test, dtype=dtype).float()\n",
    "\n",
    "#################################################################################\n",
    "#                          END OF THE CODE                                      #\n",
    "#################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 - Setting up the neural network\n",
    "\n",
    "Below you can find the setup recommended by Fernandez et al., 2006 (https://doi.org/10.1002/ceat.200500310) for the optimization of the FTS process. \n",
    "\n",
    "Now with the tensors being properly defined, we can build and train the FFNN for predicting the product distribution from FTS at 523 K. Fernandes et al., 2006 suggested that a topology of 20-25-25 for three hidden layers was suitable. Thus, we will give it a try. You can tune the number of nodes per layer, the number of hidden layers, and the learning rate. In order to assess the accuracy of your model, we calculate the MSE for the test set during training and testing. \n",
    "\n",
    "In this task, you invite you to tune the following variables and explore the results upon the change:\n",
    "\n",
    "* `n_input`, `n_output`, `n_nodes` and `epochs` used for your FF-NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (architecture): Sequential(\n",
       "    (0): Linear(in_features=3, out_features=20, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=20, out_features=25, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=25, out_features=25, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=25, out_features=8, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): Linear(in_features=8, out_features=8, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining the neural network class for this problem\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, n_input, n_hidden, n_output):\n",
    "\n",
    "        ''' Sets up the layout of the neural network.  \n",
    "        \n",
    "        n_iput: number of input neurons\n",
    "        n_hidden: number of neurons in the hidden layers\n",
    "        n_output: number of output neurons\n",
    "\n",
    "        '''\n",
    "        # Unpacking topology\n",
    "\n",
    "        n_hidden1, n_hidden2, n_hidden3 = n_hidden      #unpacking topology of layers\n",
    "\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.architecture = nn.Sequential(\n",
    "            # sequential model definition: add up layers & activation functions\n",
    "            nn.Linear(in_features=n_input, out_features=n_hidden1, bias=True),  # input layer\n",
    "            nn.ReLU(), # activation function\n",
    "            \n",
    "            # First hidden layer\n",
    "\n",
    "            nn.Linear(in_features=n_hidden1, out_features=n_hidden2, bias=True),  # hidden layer\n",
    "            nn.ReLU(), # activation function of hidden layer\n",
    "\n",
    "            # Second hidden layer\n",
    "\n",
    "            nn.Linear(in_features=n_hidden2, out_features=n_hidden3, bias=True),  # hidden layer\n",
    "            nn.ReLU(), # activation function of hidden layer\n",
    "\n",
    "        # Third hidden layer\n",
    "\n",
    "            nn.Linear(in_features=n_hidden3, out_features=n_output, bias=True),  # hidden layer\n",
    "            nn.ReLU(), # activation function of hidden layer\n",
    "\n",
    "            # Output layer\n",
    "            nn.Linear(in_features=n_output, out_features=n_output, bias=True),   # output layer\n",
    "\n",
    "        )\n",
    "    def forward(self, input): # feed forward path\n",
    "        output = self.architecture(input)\n",
    "        return output\n",
    "\n",
    "hidden_size = [20, 25, 25] # number of neurons in h1, h2, h3\n",
    "learning_rate = 0.5e-3\n",
    "\n",
    "# neural network training\n",
    "net = NeuralNetwork(3, hidden_size, 8) # create instance of neural network (inputs, hidden layer topology, outputs)\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate) # choose optimizer and learning rate\n",
    "loss_fun = nn.MSELoss() # define loss function\n",
    "epochs = 150000 # set number of epochs\n",
    "net             # prints the network topology\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 - Training the network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150000/150000 [01:40<00:00, 1489.20it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final training loss: \n",
      "0.005832319147884846\n",
      "Hidden layer size:  [20, 25, 25]\n",
      "Learning rate:  0.0005\n",
      "Validation MSE: 0.05\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "# Lists to store the loss values during training and testing\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "\n",
    "# train the network\n",
    "for epoch in tqdm(range(epochs)):\n",
    "\n",
    "    # training data\n",
    "    optimizer.zero_grad() # clear gradients for next training epoch\n",
    "    y_pred = net(xt_train) # forward pass: prediction y based on input x\n",
    "    loss = loss_fun(y_pred, yt_train)  # compare true y and predicted y to get the loss\n",
    "    loss.backward() # backpropagation, compute gradients\n",
    "    optimizer.step() # apply gradients to update weights and biases\n",
    "    train_loss.append(loss.item()) # save loss for later evaluation\n",
    "\n",
    "    # validation data\n",
    "    y_val = net(xt_test) # prediction of y based on input from validation set\n",
    "    loss = loss_fun(y_val, yt_test) # compare true y and predicted y to get the loss\n",
    "    val_loss.append(loss.item()) # save loss for later evaluation\n",
    "\n",
    "# Training and validation graph\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(train_loss, label='Training')\n",
    "plt.plot(val_loss, label='validation', linestyle='--')\n",
    "#plt.yscale('log')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.title('Loss plot')\n",
    "plt.show()\n",
    "print('Final training loss: ')\n",
    "print(train_loss[-1])\n",
    "\n",
    "# Summary\n",
    "\n",
    "print('Hidden layer size: ', hidden_size)\n",
    "print('Learning rate: ', learning_rate)\n",
    "print('Validation MSE: {:.2f}'.format(val_loss[-1])) # last element from the validation loss\n",
    "\n",
    "#################################################################################\n",
    "#                          END OF THE CODE                                      #\n",
    "#################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7 - Assessing our predictions \n",
    "\n",
    "Fernandes et al., assessed the predictions from the NN by using the absolute and relative errors of the product distribution (i.e., OPE and MPE, respectively). \n",
    "Additionally, after training their model, they suggest some optimal conditions for obtaining the maximum yields of gasoline, Diesel, and Diesel with olefins with C9+ molecules. For our puposes, first we will compare the prediction with the results obtained for the first run reported in Table 1 (Fernandes et al., 2006). \n",
    "\n",
    "* ***Presure [MPa]*** : 1.5\n",
    "* ***$H_{2}$:$CO$ ratio [-]*** : 2.0\n",
    "* ***$\\phi$/W [$10^{3}$ N $m^{3}$ $g_{{cat}}^{-1}$ $s^{-1}$]***: 1.5 (i.e., space velocity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The operating conditions : \n",
      "       Pressure: 1.50 MPa \n",
      "       H2:CO ratio: 2.00  \n",
      "       Space velocity: 1.50 \n",
      "       result in the following values for: \n",
      "        - OPE: 5.74 \n",
      "        - MPE 16.53 %\n"
     ]
    }
   ],
   "source": [
    "# Defining functions for computing the error \n",
    "\n",
    "def rsme(prediction, true_result):\n",
    "    return np.sqrt(((prediction - true_result)**2).mean())\n",
    "\n",
    "def ope(prediction, true_result):\n",
    "    ''' Computes the absolute error of the predictions '''\n",
    "\n",
    "    return np.sum(np.abs(prediction - true_result))\n",
    "\n",
    "def mpe(prediction, true_result):\n",
    "    ''' Relative error [%] according to Fernandes et al., 2006 '''\n",
    "\n",
    "    return np.sum(np.abs(prediction - true_result)/true_result)/np.size(true_result)*100\n",
    "\n",
    "# Comparison of the prediction and ground truth values for the 1st run in Fernandes et al., 2006\n",
    "op_conditions = np.array([1.5, 2.0, 1.5]).reshape(-1,1)\n",
    "\n",
    "# Normalization of operational conditions\n",
    "norm_op_conditions = norm_entries.transform(op_conditions.T)  # The normalized entries\n",
    "\n",
    "# Converting operational conditions to tensor\n",
    "xt_op = torch.tensor(norm_op_conditions, dtype=torch.float)\n",
    "\n",
    "# Predicting product distribution with NN\n",
    "with torch.no_grad():\n",
    "    sol = net(xt_op)\n",
    "\n",
    "# Extracting the prediction\n",
    "prediction = sol.detach().numpy()\n",
    "rescaled_prediction = norm_output.inverse_transform(prediction) # Rescaling the output\n",
    "\n",
    "# Result suggested by Fernandes et al., 2006\n",
    "# Order: xco, LG, LPG, Gasoline, Diesel, O2+, O4+, O9+\n",
    "expected_result = np.array(([46.2, 0.226, 0.069, 0.101, 0.070, 0.140, 0.339, 0.055]))\n",
    "\n",
    "# Calculating the RSME of the prediction\n",
    "error = rsme(rescaled_prediction, expected_result)\n",
    "\n",
    "#Calculating the MPE and OPE according to the definitions of Fernandes et al., 2006\n",
    "error_mpe = mpe(rescaled_prediction, expected_result)\n",
    "error_ope = ope(rescaled_prediction, expected_result)\n",
    "\n",
    "# Printing the results \n",
    "print('The operating conditions : \\n \\\n",
    "      Pressure: {:.2f} MPa \\n \\\n",
    "      H2:CO ratio: {:.2f}  \\n \\\n",
    "      Space velocity: {:.2f} \\n \\\n",
    "      result in the following values for: \\n \\\n",
    "       - OPE: {:.2f} \\n \\\n",
    "       - MPE {:.2f} %'.format(op_conditions[0,0], \n",
    "                              op_conditions[1,0], \n",
    "                              op_conditions[2,0], \n",
    "                               error_ope, error_mpe))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, after training their model, they suggest some optimal conditions for obtaining the maximum yields of gasoline, Diesel, and Diesel with olefins with C9+ molecules. For our puposes, we will test the conditions they reported for maximizing the yield of gasoline: \n",
    "\n",
    "* ***Presure [MPa]*** : 2.51\n",
    "* ***$H_{2}$:$CO$ ratio [-]*** : 1.75\n",
    "* ***$\\phi$/W [$10^{3}$ N $m^{3}$ $g_{{cat}}^{-1}$ $s^{-1}$]***: 0.50 (i.e., space velocity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The operating conditions : \n",
      "       Pressure: 1.50 MPa \n",
      "       H2:CO ratio: 2.00  \n",
      "       Space velocity: 1.50 \n",
      "       result in the following values for: \n",
      "        - OPE: 3.81 \n",
      "        - MPE 1.61 %\n"
     ]
    }
   ],
   "source": [
    "# Optimal conditions reported by Fernandes et al., 2006 (P [MPa], H2:CO ratio, phi_cat [10^3 N m3 / g_cat / s])\n",
    "opt_conditions = torch.tensor(np.array(([2.51, 1.75, 0.50])), dtype=torch.float).unsqueeze(-1)\n",
    "opt_conditions.shape\n",
    "\n",
    "# Normalizing the operating conditions for the NN\n",
    "norm_opt_conditions = norm_entries.transform(opt_conditions.T)  # The normalized entries\n",
    "\n",
    "# Creates the tensor of the optimal conditions\n",
    "xt_opt = torch.tensor(norm_opt_conditions, dtype=torch.float)\n",
    "\n",
    "# Implementing the network to predict product distribution\n",
    "with torch.no_grad():\n",
    "    sol = net(xt_opt)\n",
    "\n",
    "# Extracting the prediction and rescale it \n",
    "prediction = sol.detach().numpy()\n",
    "rescaled_prediction = norm_output.inverse_transform(prediction)\n",
    "\n",
    "# Post-processing the prediction according to the variables reported by Fernandes et al., 2006\n",
    "# i.e., olefins O2+ and O4+ are combined \n",
    "opt_prediction = np.append(rescaled_prediction[0,:4], \n",
    "                                [rescaled_prediction[0,5]+ rescaled_prediction[0,6],\n",
    "                                rescaled_prediction[0,7]])\n",
    "\n",
    "# Results for the optimum conditions for producing gasoline according to Fernandes et al., 2006\n",
    "# i.e., Xco, LG, LPG, Gasoline, Diesel, O2+ + O4+ , O9+\n",
    "expected_result = np.array(([80.3, 0.287, 0.124, 0.156, 0.308, 0.017])) \n",
    "\n",
    "# Calculating OPE\n",
    "error_ope = ope(opt_prediction, expected_result)\n",
    "\n",
    "# Calculating MPE\n",
    "error_mpe = mpe(opt_prediction, expected_result)\n",
    "\n",
    "# Printing the results \n",
    "print('The operating conditions : \\n \\\n",
    "      Pressure: {:.2f} MPa \\n \\\n",
    "      H2:CO ratio: {:.2f}  \\n \\\n",
    "      Space velocity: {:.2f} \\n \\\n",
    "      result in the following values for: \\n \\\n",
    "       - OPE: {:.2f} \\n \\\n",
    "       - MPE {:.2f} %'.format(op_conditions[0,0], \n",
    "                              op_conditions[1,0], \n",
    "                              op_conditions[2,0], \n",
    "                               error_ope, error_mpe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8 - Summary\n",
    "\n",
    "We showed how a simple NN can be set and how it can be implemented to derive a model for predicting the product distribution of a FTS conducted at 523 K. The results are close to the reported ones, but for the last fraction of olefins (O9+). This indicates that the topology require a fine-tunning of the hyperparameters. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
